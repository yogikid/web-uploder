package main

import (
	"bytes"
	"context"
	"crypto/aes"
	"crypto/cipher"
	"crypto/rand"
	"crypto/sha256"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"mime"
	"net/http"
	"os"
	"os/signal"
	"path/filepath"
	"runtime"
	"strings"
	"sync"
	"syscall"
	"time"
	"unsafe"

	"github.com/aws/aws-sdk-go-v2/aws"
	awsconfig "github.com/aws/aws-sdk-go-v2/config"
	"github.com/aws/aws-sdk-go-v2/credentials"
	"github.com/aws/aws-sdk-go-v2/service/s3"
	"github.com/gofiber/fiber/v2"
	"github.com/gofiber/fiber/v2/middleware/compress"
	"github.com/gofiber/fiber/v2/middleware/cors"
	"github.com/google/uuid"
	"github.com/redis/go-redis/v9"
	"golang.org/x/oauth2"
	"golang.org/x/oauth2/google"
	"google.golang.org/api/drive/v3"
	"google.golang.org/api/googleapi"
	"google.golang.org/api/option"
)

const (
	ChunkSize     = 1024 * 1024 * 16 // 16MB chunks
	BufferSize    = 1024 * 1024 * 8  // 8MB buffer
	MaxWorkers    = 16
	MaxUploads    = 4
	PaddingFactor = 3
)

type Config struct {
	// Google Drive
	ClientID     string `json:"client_id"`
	ClientSecret string `json:"client_secret"`
	RedirectURI  string `json:"redirect_uri"`
	RefreshToken string `json:"refresh_token"`
	FolderID     string `json:"folder_id"`

	// Redis
	RedisAddr     string `json:"redis_addr"`
	RedisPassword string `json:"redis_password"`
	RedisDB       int    `json:"redis_db"`

	// S3
	S3Endpoint        string `json:"s3_endpoint"`
	S3Region          string `json:"s3_region"`
	S3Bucket          string `json:"s3_bucket"`
	S3AccessKeyID     string `json:"s3_access_key_id"`
	S3SecretAccessKey string `json:"s3_secret_access_key"`

	// Storage Selection
	StorageBackend string `json:"storage_backend"` // "drive" or "s3"
}

type FileRecord struct {
	ID         string    `json:"id"`
	DriveID    string    `json:"drive_id"` // Used for both Drive ID and S3 Key
	Password   string    `json:"password"`
	FileInfo   FileInfo  `json:"file_info"`
	UploadedAt time.Time `json:"uploaded_at"`
	FakeName   string    `json:"fake_name"`
	RealSize   int64     `json:"real_size"`
}

type FileInfo struct {
	Name      string `json:"name"`
	Extension string `json:"extension"`
	Size      int64  `json:"size"`
	MimeType  string `json:"mime_type"`
}

type UploadStats struct {
	StartTime   time.Time `json:"start_time"`
	EndTime     time.Time `json:"end_time"`
	Duration    string    `json:"duration"`
	TotalSize   int64     `json:"total_size"`
	UploadSpeed string    `json:"upload_speed"`
}

type DriveQuota struct {
	Used  int64 `json:"used"`
	Total int64 `json:"total"`
	Free  int64 `json:"free"`
}

// Interfaces

type StorageProvider interface {
	Upload(ctx context.Context, data []byte, filename string) (string, error)
	Download(ctx context.Context, id string) ([]byte, error)
	GetQuota(ctx context.Context) (*DriveQuota, error)
	Name() string
}

type MetadataStore interface {
	Save(ctx context.Context, record FileRecord) error
	Get(ctx context.Context, id string) (FileRecord, error)
	Close() error
}

// Implementations

// RedisMetadataStore
type RedisMetadataStore struct {
	client *redis.Client
}

func NewRedisMetadataStore(config Config) *RedisMetadataStore {
	rdb := redis.NewClient(&redis.Options{
		Addr:     config.RedisAddr,
		Password: config.RedisPassword,
		DB:       config.RedisDB,
	})
	return &RedisMetadataStore{client: rdb}
}

func (s *RedisMetadataStore) Save(ctx context.Context, record FileRecord) error {
	data, err := json.Marshal(record)
	if err != nil {
		return err
	}
	return s.client.Set(ctx, "file:"+record.ID, data, 0).Err()
}

func (s *RedisMetadataStore) Get(ctx context.Context, id string) (FileRecord, error) {
	val, err := s.client.Get(ctx, "file:"+id).Result()
	if err != nil {
		return FileRecord{}, err
	}
	var record FileRecord
	err = json.Unmarshal([]byte(val), &record)
	return record, err
}

func (s *RedisMetadataStore) Close() error {
	return s.client.Close()
}

// GoogleDriveStorage
type GoogleDriveStorage struct {
	service  *drive.Service
	folderID string
}

func NewGoogleDriveStorage(ctx context.Context, config Config) (*GoogleDriveStorage, error) {
	oauth2Config := &oauth2.Config{
		ClientID:     config.ClientID,
		ClientSecret: config.ClientSecret,
		RedirectURL:  config.RedirectURI,
		Scopes:       []string{drive.DriveScope},
		Endpoint:     google.Endpoint,
	}

	token := &oauth2.Token{RefreshToken: config.RefreshToken}
	client := oauth2Config.Client(ctx, token)
	client.Timeout = 5 * time.Minute

	service, err := drive.NewService(ctx, option.WithHTTPClient(client))
	if err != nil {
		return nil, err
	}

	return &GoogleDriveStorage{
		service:  service,
		folderID: config.FolderID,
	}, nil
}

func (s *GoogleDriveStorage) Upload(ctx context.Context, data []byte, filename string) (string, error) {
	file := &drive.File{
		Name:    filename,
		Parents: []string{s.folderID},
	}

	reader := bytes.NewReader(data)
	call := s.service.Files.Create(file).Context(ctx).Media(reader, googleapi.ChunkSize(ChunkSize))

	res, err := call.Do()
	if err != nil {
		return "", err
	}
	return res.Id, nil
}

func (s *GoogleDriveStorage) Download(ctx context.Context, id string) ([]byte, error) {
	resp, err := s.service.Files.Get(id).Context(ctx).Download()
	if err != nil {
		return nil, err
	}
	defer resp.Body.Close()
	return io.ReadAll(resp.Body)
}

func (s *GoogleDriveStorage) GetQuota(ctx context.Context) (*DriveQuota, error) {
	about, err := s.service.About.Get().Fields("storageQuota").Do()
	if err != nil {
		return nil, err
	}

	return &DriveQuota{
		Used:  about.StorageQuota.Usage,
		Total: about.StorageQuota.Limit,
		Free:  about.StorageQuota.Limit - about.StorageQuota.Usage,
	}, nil
}

func (s *GoogleDriveStorage) Name() string {
	return "Google Drive"
}

// S3Storage
type S3Storage struct {
	client *s3.Client
	bucket string
}

func NewS3Storage(ctx context.Context, cfg Config) (*S3Storage, error) {
	awsCfg, err := awsconfig.LoadDefaultConfig(ctx,
		awsconfig.WithRegion(cfg.S3Region),
		awsconfig.WithCredentialsProvider(credentials.NewStaticCredentialsProvider(cfg.S3AccessKeyID, cfg.S3SecretAccessKey, "")),
	)
	if err != nil {
		return nil, err
	}

	// If custom endpoint is provided (e.g. MinIO, R2, etc)
	if cfg.S3Endpoint != "" {
		awsCfg.BaseEndpoint = aws.String(cfg.S3Endpoint)
	}

	client := s3.NewFromConfig(awsCfg)

	return &S3Storage{
		client: client,
		bucket: cfg.S3Bucket,
	}, nil
}

func (s *S3Storage) Upload(ctx context.Context, data []byte, filename string) (string, error) {
	// For S3, we can use the filename (which is the fake name) as the key
	key := filename

	_, err := s.client.PutObject(ctx, &s3.PutObjectInput{
		Bucket: aws.String(s.bucket),
		Key:    aws.String(key),
		Body:   bytes.NewReader(data),
	})
	if err != nil {
		return "", err
	}
	return key, nil
}

func (s *S3Storage) Download(ctx context.Context, id string) ([]byte, error) {
	// id is the Key
	out, err := s.client.GetObject(ctx, &s3.GetObjectInput{
		Bucket: aws.String(s.bucket),
		Key:    aws.String(id),
	})
	if err != nil {
		return nil, err
	}
	defer out.Body.Close()
	return io.ReadAll(out.Body)
}

func (s *S3Storage) GetQuota(ctx context.Context) (*DriveQuota, error) {
	// S3 doesn't easily provide quota info. Returning dummy or max.
	return &DriveQuota{
		Used:  0,
		Total: -1, // -1 indicating unlimited or unknown
		Free:  -1,
	}, nil
}

func (s *S3Storage) Name() string {
	return "S3"
}

type FileUploader struct {
	storage       StorageProvider
	metadata      MetadataStore
	config        Config
	memoryCache   sync.Map
	ctx           context.Context
	cancel        context.CancelFunc
	uploadPool    chan struct{}
	bufferPool    sync.Pool
}

var fakeNames = []string{
	"system_backup.tmp", "cache_data.bin", "temp_file.dat",
	"log_archive.tmp", "buffer_swap.bin", "memory_dump.dat",
	"config_cache.tmp", "process_data.bin", "kernel_dump.dat",
	"registry_backup.tmp", "thread_dump.bin", "heap_data.dat",
	"driver_cache.tmp", "service_log.bin", "event_data.dat",
	"boot_config.tmp", "system_info.bin", "device_dump.dat",
	"network_cache.tmp", "socket_data.bin", "protocol_dump.dat",
	"security_log.tmp", "auth_cache.bin", "session_data.dat",
	"resource_dump.tmp", "handle_cache.bin", "object_data.dat",
	"scheduler_log.tmp", "queue_data.bin", "task_dump.dat",

	"app_config.xml", "user_settings.json", "preferences.ini",
	"database_backup.sql", "migration_log.txt", "schema_dump.db",
	"api_response.json", "request_log.txt", "endpoint_cache.xml",
	"session_store.dat", "cookie_cache.tmp", "token_data.bin",
	"upload_temp.file", "download_cache.tmp", "media_buffer.dat",
	"thumbnail_cache.jpg", "preview_data.png", "image_temp.bmp",
	"video_buffer.mp4", "audio_cache.wav", "stream_data.tmp",
	"plugin_config.dll", "extension_data.so", "module_cache.dylib",
	"theme_data.css", "style_cache.scss", "layout_temp.html",
	"script_cache.js", "library_data.min.js", "framework_temp.ts",

	"document_backup.docx", "spreadsheet_temp.xlsx", "presentation_cache.pptx",
	"pdf_buffer.pdf", "text_backup.txt", "note_cache.md",
	"template_data.dotx", "form_backup.xltx", "slide_temp.potx",
	"report_cache.doc", "analysis_temp.xls", "summary_data.ppt",
	"contract_backup.pdf", "invoice_temp.xlsx", "receipt_cache.doc",
	"proposal_data.docx", "budget_temp.xls", "forecast_cache.xlsx",
	"manual_backup.pdf", "guide_temp.doc", "tutorial_cache.txt",
	"readme_data.md", "changelog_temp.txt", "version_cache.log",
	"license_backup.txt", "terms_temp.pdf", "policy_cache.doc",
	"specification_data.docx", "requirement_temp.xlsx", "design_cache.ppt",

	"source_backup.cpp", "header_cache.h", "library_temp.lib",
	"binary_data.exe", "object_cache.o", "assembly_temp.asm",
	"makefile_backup.mk", "script_cache.sh", "batch_temp.bat",
	"config_data.cmake", "project_cache.vcxproj", "solution_temp.sln",
	"package_backup.json", "dependency_cache.lock", "module_temp.py",
	"class_data.java", "interface_cache.kt", "struct_temp.rs",
	"function_backup.js", "method_cache.ts", "procedure_temp.sql",
	"query_data.sql", "view_cache.sql", "trigger_temp.sql",
	"index_backup.sql", "constraint_cache.sql", "procedure_temp.sql",
	"migration_data.rb", "seed_cache.sql", "fixture_temp.yml",

	"image_backup.jpg", "photo_cache.png", "picture_temp.gif",
	"video_data.mp4", "movie_cache.avi", "clip_temp.mov",
	"audio_backup.mp3", "music_cache.wav", "sound_temp.ogg",
	"recording_data.m4a", "podcast_cache.mp3", "voice_temp.wav",
	"graphics_backup.svg", "vector_cache.ai", "design_temp.psd",
	"icon_data.ico", "logo_cache.png", "banner_temp.jpg",
	"animation_backup.gif", "sprite_cache.png", "texture_temp.tga",
	"model_data.obj", "mesh_cache.fbx", "scene_temp.blend",
	"font_backup.ttf", "typeface_cache.otf", "glyph_temp.woff",
	"document_scan.pdf", "page_cache.tiff", "archive_temp.zip",

	"dataset_backup.csv", "records_cache.json", "entries_temp.xml",
	"database_dump.sql", "table_cache.db", "collection_temp.bson",
	"index_data.idx", "search_cache.lucene", "query_temp.elasticsearch",
	"log_backup.log", "trace_cache.txt", "debug_temp.out",
	"metrics_data.json", "stats_cache.csv", "analytics_temp.xml",
	"config_backup.yaml", "settings_cache.toml", "properties_temp.ini",
	"registry_data.reg", "preference_cache.plist", "option_temp.conf",
	"certificate_backup.crt", "key_cache.pem", "token_temp.jwt",
	"hash_data.md5", "checksum_cache.sha256", "signature_temp.sig",
	"backup_archive.tar.gz", "compressed_cache.zip", "packed_temp.rar",

	"webpage_backup.html", "style_cache.css", "script_temp.js",
	"component_data.jsx", "template_cache.vue", "module_temp.ts",
	"api_backup.json", "response_cache.xml", "request_temp.http",
	"session_data.cookie", "storage_cache.localStorage", "temp_sessionStorage",
	"manifest_backup.json", "worker_cache.js", "service_temp.sw.js",
	"bundle_data.js", "chunk_cache.js", "vendor_temp.js",
	"asset_backup.css", "resource_cache.scss", "theme_temp.less",
	"font_data.woff2", "icon_cache.svg", "image_temp.webp",
	"config_backup.webpack.js", "build_cache.rollup.js", "compile_temp.babel.js",
	"package_data.npm", "dependency_cache.yarn.lock", "module_temp.node_modules",

	"backup_full.zip", "archive_daily.tar", "compressed_weekly.rar",
	"snapshot_data.7z", "backup_incremental.gz", "archive_monthly.bz2",
	"system_backup.tar.gz", "data_archive.zip", "file_backup.rar",
	"config_snapshot.tar", "setting_backup.7z", "preference_archive.gz",
	"database_backup.sql.gz", "log_archive.tar.bz2", "temp_backup.zip",
	"user_data.backup", "profile_archive.tar", "session_backup.gz",
	"cache_snapshot.zip", "temp_archive.rar", "buffer_backup.7z",
	"memory_dump.tar.gz", "crash_backup.zip", "error_archive.tar",
	"debug_snapshot.gz", "trace_backup.bz2", "log_archive.7z",
	"metrics_backup.zip", "stats_archive.tar", "report_backup.gz",

	"processing_temp.tmp", "workflow_cache.wf", "pipeline_data.pipe",
	"batch_process.batch", "queue_item.queue", "job_data.job",
	"task_temp.task", "worker_cache.work", "thread_data.thread",
	"process_buffer.proc", "execution_temp.exec", "runtime_cache.run",
	"compile_temp.build", "link_cache.link", "deploy_temp.deploy",
	"test_data.test", "mock_cache.mock", "stub_temp.stub",
	"fixture_data.fixture", "sample_cache.sample", "demo_temp.demo",
	"prototype_data.proto", "template_cache.tmpl", "pattern_temp.pattern",
	"schema_data.schema", "model_cache.model", "entity_temp.entity",
	"service_data.service", "handler_cache.handler", "controller_temp.ctrl",
	"middleware_data.middleware", "filter_cache.filter", "interceptor_temp.int",

	"utility_backup.util", "helper_cache.help", "tool_temp.tool",
	"script_data.script", "command_cache.cmd", "function_temp.func",
	"library_backup.lib", "module_cache.mod", "package_temp.pkg",
	"plugin_data.plugin", "extension_cache.ext", "addon_temp.addon",
	"component_backup.comp", "widget_cache.widget", "control_temp.ctrl",
	"service_data.svc", "daemon_cache.daemon", "agent_temp.agent",
	"monitor_backup.mon", "watcher_cache.watch", "observer_temp.obs",
	"listener_data.listen", "handler_cache.handle", "processor_temp.proc",
	"parser_backup.parse", "validator_cache.valid", "formatter_temp.fmt",
	"converter_data.conv", "transformer_cache.trans", "mapper_temp.map",
	"serializer_backup.serial", "encoder_cache.encode", "decoder_temp.decode",

	"network_config.net", "connection_cache.conn", "socket_temp.sock",
	"protocol_data.proto", "packet_cache.packet", "frame_temp.frame",
	"message_backup.msg", "request_cache.req", "response_temp.resp",
	"header_data.header", "payload_cache.payload", "body_temp.body",
	"session_backup.session", "token_cache.token", "auth_temp.auth",
	"certificate_data.cert", "key_cache.key", "signature_temp.sig",
	"encryption_backup.enc", "hash_cache.hash", "digest_temp.digest",
	"checksum_data.chk", "validation_cache.valid", "verification_temp.verify",
	"firewall_backup.fw", "security_cache.sec", "access_temp.access",
	"permission_data.perm", "privilege_cache.priv", "right_temp.right",
	"policy_backup.policy", "rule_cache.rule", "condition_temp.cond",

	"security_log.sec", "audit_cache.audit", "compliance_temp.comp",
	"vulnerability_data.vuln", "threat_cache.threat", "risk_temp.risk",
	"scan_backup.scan", "analysis_cache.analysis", "report_temp.report",
	"incident_data.incident", "alert_cache.alert", "warning_temp.warn",
	"forensic_backup.forensic", "evidence_cache.evidence", "trace_temp.trace",
	"malware_data.malware", "virus_cache.virus", "trojan_temp.trojan",
	"quarantine_backup.quar", "sandbox_cache.sandbox", "isolation_temp.iso",
	"backup_encrypted.enc", "archive_secured.sec", "data_protected.prot",
	"file_locked.lock", "content_sealed.seal", "information_hidden.hide",
	"secret_data.secret", "private_cache.private", "confidential_temp.conf",
	"classified_backup.class", "restricted_cache.restrict", "limited_temp.limit",

	"monitor_log.monitor", "performance_cache.perf", "benchmark_temp.bench",
	"metrics_data.metrics", "statistics_cache.stats", "analytics_temp.analytics",
	"profiling_backup.profile", "tracing_cache.trace", "debugging_temp.debug",
	"diagnostic_data.diag", "health_cache.health", "status_temp.status",
	"uptime_backup.uptime", "availability_cache.avail", "reliability_temp.rel",
	"load_data.load", "capacity_cache.capacity", "usage_temp.usage",
	"resource_backup.resource", "memory_cache.memory", "cpu_temp.cpu",
	"disk_data.disk", "network_cache.network", "io_temp.io",
	"bandwidth_backup.bandwidth", "latency_cache.latency", "throughput_temp.through",
	"response_data.response", "request_cache.request", "transaction_temp.trans",

	"app_config.config", "system_settings.settings", "user_preferences.prefs",
	"environment_vars.env", "runtime_config.runtime", "deployment_settings.deploy",
	"database_config.db.config", "server_settings.server", "client_config.client",
	"api_settings.api", "service_config.service", "module_settings.module",
	"plugin_config.plugin", "theme_settings.theme", "locale_config.locale",
	"cache_settings.cache", "session_config.session", "cookie_settings.cookie",
	"security_config.security", "auth_settings.auth", "permission_config.permission",
	"logging_settings.logging", "debug_config.debug", "error_settings.error",
	"backup_config.backup", "archive_settings.archive", "restore_config.restore",
	"sync_settings.sync", "replication_config.replication", "mirror_settings.mirror",
	"cluster_config.cluster", "node_settings.node", "shard_config.shard",
}

func NewFileUploader(configPath string) (*FileUploader, error) {
	configFile, err := os.ReadFile(configPath)
	if err != nil {
		return nil, err
	}

	var config Config
	if err := json.Unmarshal(configFile, &config); err != nil {
		return nil, err
	}

	ctx, cancel := context.WithCancel(context.Background())

	var storage StorageProvider
	if strings.ToLower(config.StorageBackend) == "s3" {
		storage, err = NewS3Storage(ctx, config)
	} else {
		// Default to Drive
		storage, err = NewGoogleDriveStorage(ctx, config)
	}
	if err != nil {
		cancel()
		return nil, err
	}

	metadata := NewRedisMetadataStore(config)

	fu := &FileUploader{
		storage:    storage,
		metadata:   metadata,
		config:     config,
		ctx:        ctx,
		cancel:     cancel,
		uploadPool: make(chan struct{}, MaxUploads),
		bufferPool: sync.Pool{
			New: func() interface{} {
				return make([]byte, BufferSize)
			},
		},
	}

	runtime.GC()
	go fu.handleShutdown()

	return fu, nil
}

func (fu *FileUploader) handleShutdown() {
	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)
	<-sigChan
	fu.metadata.Close()
	time.Sleep(1 * time.Second)
	fu.cancel()
	os.Exit(0)
}

func generateSecurePassword() string {
	b := make([]byte, 24)
	rand.Read(b)
	const chars = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789!@#$%^&*"
	for i, v := range b {
		b[i] = chars[v%byte(len(chars))]
	}
	return *(*string)(unsafe.Pointer(&b))
}

func getFakeName() string {
	return fakeNames[time.Now().UnixNano()%int64(len(fakeNames))]
}

func (fu *FileUploader) advancedEncrypt(data []byte, password string) ([]byte, error) {
	key := sha256.Sum256([]byte(password))
	block, err := aes.NewCipher(key[:])
	if err != nil {
		return nil, err
	}

	gcm, err := cipher.NewGCM(block)
	if err != nil {
		return nil, err
	}

	nonce := make([]byte, gcm.NonceSize())
	rand.Read(nonce)

	encrypted := gcm.Seal(nonce, nonce, data, nil)

	paddingSize := len(encrypted) * PaddingFactor
	padding := make([]byte, paddingSize)
	rand.Read(padding)

	fakeHeaders := []byte{
		0x50, 0x4B, 0x03, 0x04, 0x14, 0x00, 0x00, 0x00, 0x08, 0x00,
		0x21, 0x0C, 0x4B, 0x50, 0x28, 0xB5, 0x2F, 0xFD, 0x87, 0x00,
		0x00, 0x00, 0x75, 0x00, 0x00, 0x00, 0x08, 0x00, 0x1C, 0x00,
	}

	result := make([]byte, 0, len(fakeHeaders)+len(padding)+len(encrypted)+1024)
	result = append(result, fakeHeaders...)
	result = append(result, padding...)
	result = append(result, encrypted...)

	extraNoise := make([]byte, 1024)
	rand.Read(extraNoise)
	result = append(result, extraNoise...)

	return result, nil
}

func (fu *FileUploader) advancedDecrypt(data []byte, password string) ([]byte, error) {
	if len(data) < 1024+30 {
		return nil, fmt.Errorf("invalid data")
	}

	headerSize := 30
	originalLen := len(data) - headerSize - 1024
	paddingSize := originalLen * PaddingFactor / (PaddingFactor + 1)
	encryptedStart := headerSize + paddingSize
	encryptedEnd := encryptedStart + (originalLen - paddingSize)

	if encryptedEnd > len(data)-1024 {
		return nil, fmt.Errorf("data corruption")
	}

	encrypted := data[encryptedStart:encryptedEnd]

	key := sha256.Sum256([]byte(password))
	block, err := aes.NewCipher(key[:])
	if err != nil {
		return nil, err
	}

	gcm, err := cipher.NewGCM(block)
	if err != nil {
		return nil, err
	}

	if len(encrypted) < gcm.NonceSize() {
		return nil, fmt.Errorf("invalid encrypted data")
	}

	nonce, ciphertext := encrypted[:gcm.NonceSize()], encrypted[gcm.NonceSize():]
	return gcm.Open(nil, nonce, ciphertext, nil)
}

func isMediaFile(mimeType string) bool {
	return strings.HasPrefix(mimeType, "video/") ||
		strings.HasPrefix(mimeType, "image/") ||
		strings.HasPrefix(mimeType, "audio/")
}

func (fu *FileUploader) handleUpload(c *fiber.Ctx) error {
	startTime := time.Now()

	form, err := c.MultipartForm()
	if err != nil {
		return c.Status(400).JSON(fiber.Map{"error": "Invalid form"})
	}

	fileHeaders := form.File["files"]
	if len(fileHeaders) != 1 {
		return c.Status(400).JSON(fiber.Map{"error": "Single file only"})
	}

	fileHeader := fileHeaders[0]
	if fileHeader.Size > 50*1024*1024 {
		return c.Status(413).JSON(fiber.Map{"error": "File too large"})
	}

	file, err := fileHeader.Open()
	if err != nil {
		return c.Status(400).JSON(fiber.Map{"error": "Cannot open file"})
	}
	defer file.Close()

	data := make([]byte, fileHeader.Size)
	_, err = io.ReadFull(file, data)
	if err != nil {
		return c.Status(400).JSON(fiber.Map{"error": "Read failed"})
    }

	uniqueID := uuid.New().String()
	password := generateSecurePassword()
	fakeName := getFakeName()

	ext := strings.ToLower(filepath.Ext(fileHeader.Filename))
	mimeType := mime.TypeByExtension(ext)
	if mimeType == "" {
		mimeType = http.DetectContentType(data[:min(512, len(data))])
	}

	fileInfo := FileInfo{
		Name:      fileHeader.Filename,
		Extension: ext,
		Size:      fileHeader.Size,
		MimeType:  mimeType,
	}

	encryptedData, err := fu.advancedEncrypt(data, password)
	if err != nil {
		return c.Status(500).JSON(fiber.Map{"error": "Encryption failed"})
	}

    // Use semaphore to limit concurrent uploads
    fu.uploadPool <- struct{}{}
	driveID, err := fu.storage.Upload(fu.ctx, encryptedData, fakeName)
    <-fu.uploadPool

	if err != nil {
		return c.Status(500).JSON(fiber.Map{"error": "Upload failed: " + err.Error()})
	}

	endTime := time.Now()
	duration := endTime.Sub(startTime)

	record := FileRecord{
		ID:         uniqueID,
		DriveID:    driveID,
		Password:   password,
		FileInfo:   fileInfo,
		UploadedAt: startTime,
		FakeName:   fakeName,
		RealSize:   fileHeader.Size,
	}

	// Store in memory cache immediately
	fu.memoryCache.Store(uniqueID, record)

	// Store in Redis
	ctx, cancel := context.WithTimeout(fu.ctx, 10*time.Second)
	defer cancel()

	err = fu.metadata.Save(ctx, record)
	if err != nil {
		log.Printf("Metadata save error: %v", err)
	}

	stats := UploadStats{
		StartTime:   startTime,
		EndTime:     endTime,
		Duration:    duration.String(),
		TotalSize:   fileHeader.Size,
		UploadSpeed: formatSpeed(fileHeader.Size, duration),
	}

	return c.JSON(fiber.Map{
		"success": true,
		"id":      uniqueID,
		"url":     fmt.Sprintf("/file/%s", uniqueID),
		"stats":   stats,
	})
}

func (fu *FileUploader) handleFile(c *fiber.Ctx) error {
	id := c.Params("id")
	if id == "" {
		return c.Status(400).JSON(fiber.Map{"error": "ID required"})
	}

	// Try to get from memory cache first
	recordInterface, exists := fu.memoryCache.Load(id)
	var record FileRecord

	if exists {
		record = recordInterface.(FileRecord)
	} else {
		// Try to get from Redis
		ctx, cancel := context.WithTimeout(fu.ctx, 5*time.Second)
		defer cancel()

		var err error
		record, err = fu.metadata.Get(ctx, id)
		if err != nil {
			if err == redis.Nil {
				return c.Status(404).JSON(fiber.Map{"error": "File not found"})
			}
			return c.Status(500).JSON(fiber.Map{"error": "Metadata error"})
		}

		// Store back in memory cache for future requests
		fu.memoryCache.Store(id, record)
	}

    fu.uploadPool <- struct{}{}
	encryptedData, err := fu.storage.Download(fu.ctx, record.DriveID)
    <-fu.uploadPool
    
	if err != nil {
		return c.Status(500).JSON(fiber.Map{"error": "Download failed"})
	}

	data, err := fu.advancedDecrypt(encryptedData, record.Password)
	if err != nil {
		return c.Status(500).JSON(fiber.Map{"error": "Decryption failed"})
	}

	c.Set("Content-Type", record.FileInfo.MimeType)
	c.Set("Cache-Control", "public, max-age=31536000")

	if isMediaFile(record.FileInfo.MimeType) {
		c.Set("Content-Disposition", fmt.Sprintf("inline; filename=%s", record.FileInfo.Name))
	} else {
		c.Set("Content-Disposition", fmt.Sprintf("attachment; filename=%s", record.FileInfo.Name))
	}

	return c.Send(data)
}

func (fu *FileUploader) handleQuota(c *fiber.Ctx) error {
    ctx, cancel := context.WithTimeout(fu.ctx, 10*time.Second)
    defer cancel()

	quota, err := fu.storage.GetQuota(ctx)
	if err != nil {
		return c.Status(500).JSON(fiber.Map{"error": "Quota check failed"})
	}
	return c.JSON(quota)
}

func formatSpeed(bytes int64, duration time.Duration) string {
	if duration.Seconds() == 0 {
		return "âˆž B/s"
	}
	return formatBytes(int64(float64(bytes)/duration.Seconds())) + "/s"
}

func formatBytes(bytes int64) string {
	const unit = 1024
	if bytes < unit {
		return fmt.Sprintf("%d B", bytes)
    }
	div, exp := int64(unit), 0
	for n := bytes / unit; n >= unit; n /= unit {
		div *= unit
		exp++
	}
	return fmt.Sprintf("%.1f %cB", float64(bytes)/float64(div), "KMGTPE"[exp])
}

func min(a, b int) int {
	if a < b {
		return a
	}
	return b
}

func main() {
	runtime.GOMAXPROCS(runtime.NumCPU())

	uploader, err := NewFileUploader("config.json")
	if err != nil {
		log.Fatal(err)
	}

	app := fiber.New(fiber.Config{
		BodyLimit:         50 * 1024 * 1024,
		ReadTimeout:       10 * time.Minute,
		WriteTimeout:      10 * time.Minute,
		IdleTimeout:       120 * time.Second,
		Prefork:           false,
		ServerHeader:      "",
		DisableKeepalive:  false,
		StreamRequestBody: true,
	})

	app.Use(compress.New(compress.Config{
		Level: compress.LevelBestSpeed,
	}))

	app.Use(cors.New(cors.Config{
		AllowOrigins: "*",
		AllowMethods: "GET,POST,OPTIONS",
		AllowHeaders: "*",
	}))

	app.Post("/upload", uploader.handleUpload)
	app.Get("/file/:id", uploader.handleFile)
	app.Get("/quota", uploader.handleQuota)
    app.Get("/", func(c *fiber.Ctx) error {
    		return c.SendFile("./index.html")
    	})

	log.Printf("ðŸš€ Ultra-Fast Uploader running on port 5000 using %s", uploader.storage.Name())
	log.Fatal(app.Listen(":5000"))
}
